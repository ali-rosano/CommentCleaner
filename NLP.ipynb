{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09ac5b0-7996-432e-b3e3-b938bdfb5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9178ba30-0275-46c3-b940-e36991394391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"youtoxic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f37d968c-64da-438d-afdc-50b97e8975c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 CommentId      VideoId  \\\n",
       "0    Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1    Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2    Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "3    Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
       "4    Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
       "..                    ...          ...   \n",
       "995  Ugi5ADt10EdDz3gCoAEC  XRuCW80L9mA   \n",
       "996  Ugifh2DMhBbDkHgCoAEC  XRuCW80L9mA   \n",
       "997  Ugj_plbGBjjzYXgCoAEC  XRuCW80L9mA   \n",
       "998  Ugj0bah1De8xy3gCoAEC  XRuCW80L9mA   \n",
       "999  UgjBJKQSoQMQ6ngCoAEC  XRuCW80L9mA   \n",
       "\n",
       "                                                  Text  IsToxic  IsAbusive  \\\n",
       "0    If only people would just take a step back and...    False      False   \n",
       "1    Law enforcement is not trained to shoot to app...     True       True   \n",
       "2    \\nDont you reckon them 'black lives matter' ba...     True       True   \n",
       "3    There are a very large number of people who do...    False      False   \n",
       "4    The Arab dude is absolutely right, he should h...    False      False   \n",
       "..                                                 ...      ...        ...   \n",
       "995  I remember that they sent in the national defe...    False      False   \n",
       "996  Stats don`t represent the problem. Race baitin...     True      False   \n",
       "997  The quote from the mother... Wow that hit hard...    False      False   \n",
       "998                            this video is so racist    False      False   \n",
       "999      God, the narrator has such an annoying lisp.     False      False   \n",
       "\n",
       "     IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  \\\n",
       "0       False          False      False         False     False   \n",
       "1       False          False      False         False     False   \n",
       "2       False          False       True         False     False   \n",
       "3       False          False      False         False     False   \n",
       "4       False          False      False         False     False   \n",
       "..        ...            ...        ...           ...       ...   \n",
       "995     False          False      False         False     False   \n",
       "996     False          False      False          True      True   \n",
       "997     False          False      False         False     False   \n",
       "998     False          False      False         False     False   \n",
       "999     False          False      False         False     False   \n",
       "\n",
       "     IsNationalist  IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
       "0            False     False         False            False         False  \n",
       "1            False     False         False            False         False  \n",
       "2            False     False         False            False         False  \n",
       "3            False     False         False            False         False  \n",
       "4            False     False         False            False         False  \n",
       "..             ...       ...           ...              ...           ...  \n",
       "995          False     False         False            False         False  \n",
       "996          False     False         False            False         False  \n",
       "997          False     False         False            False         False  \n",
       "998          False     False         False            False         False  \n",
       "999          False     False         False            False         False  \n",
       "\n",
       "[1000 rows x 15 columns]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12160ce-c900-4db0-b751-95357c51931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Elimina caracteres especiales y puntuación\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    # Convierte a minúsculas\n",
    "    text = text.lower()\n",
    "    # Elimina números\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    # Elimina espacios en blanco adicionales\n",
    "    text = ' '.join(text.split())\n",
    "    #Reemplaza xa0 por espacios en blanco\n",
    "    df['Text'] = df['Text'].replace('\\xa0', ' ')\n",
    "    return text\n",
    "\n",
    "# Aplica la limpieza a la columna 'text'\n",
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abc709e1-a41a-4600-8d80-ce9b861e0f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      if only people would just take a step back and...\n",
       "1      law enforcement is not trained to shoot to app...\n",
       "2      dont you reckon them black lives matter banner...\n",
       "3      there are a very large number of people who do...\n",
       "4      the arab dude is absolutely right he should ha...\n",
       "                             ...                        \n",
       "995    i remember that they sent in the national defe...\n",
       "996    stats dont represent the problem race baiting ...\n",
       "997    the quote from the mother wow that hit hard ve...\n",
       "998                              this video is so racist\n",
       "999           god the narrator has such an annoying lisp\n",
       "Name: Text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d258e2f-6b5b-4256-9695-d123a93527c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['caracteres_no_alfabeticos'] = df['Text'].str.contains(r'[^a-z\\s]')\n",
    "df['caracteres_no_alfabeticos'].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b360d095-7f15-4922-8f96-91d126fdce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ec9fc6-9b3c-400c-915d-4ef31177665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      people step case nt people situation lump mess...\n",
       "1      law enforcement trained shoot apprehend traine...\n",
       "2      nt reckon black lives matter banners held whit...\n",
       "3      large number people like police officers calle...\n",
       "4      arab dude absolutely right shot extra time sho...\n",
       "                             ...                        \n",
       "995                       remember sent national defence\n",
       "996    stats nt represent problem race baiting attitu...\n",
       "997                   quote mother wow hit hard accurate\n",
       "998                                         video racist\n",
       "999                           god narrator annoying lisp\n",
       "Name: wo_stop, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "df['wo_stop'] = df['Text'].apply(remove_stopwords)\n",
    "df['wo_stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af669fd3-742f-4533-9bc7-d70b06120100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people step case nt people situation lump mess matters hands makes kinds protests selfish rational thought investigation guy video heavily emotional hyped wants heard gets heard presses reasonable discussion kudos smerconish keeping level time letting masri fool dare tore city protest dishonor entire incident hate way police brutality epidemic wish stop pretending like knew exactly going s measurable people honestly witnessed incident clue way issue swung grand jury informed trust majority rule right course action let thank police officers americathat actually serve protect bit jerk pull respect job know people going pout held accountable actions people hate police need officer emergency'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'wo_stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce1b2207-f05e-47d8-a294-301123bf346a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if only people would just take a step back and not make this case about them because it wasnt about anyone except the two people in that situation to lump yourself into this mess and take matters into your own hands makes these kinds of protests selfish and without rational thought and investigation the guy in this video is heavily emotional and hyped up and wants to be heard and when he gets heard he just presses more and more he was never out to have a reasonable discussion kudos to the smerconish for keeping level the whole time and letting masri make himself out to be a fool how dare he and those that tore that city down in protest make this about themselves and to dishonor the entire incident with their own hate by the way since when did police brutality become an epidemic i wish everyone would just stop pretending like they were there and they knew exactly what was going on because theres no measurable amount of people that honestly witnessed this incident so none of us have a clue on which way this whole issue should have swung the grand jury were the most informed we have to trust the majority rule was the right course of action and let it be also thank you to the of police officers in americathat actually serve protect even if youre a bit of a jerk when you pull me over i respect your job and know that someone has to do it and that many people are going to pout about being held accountable to their actions people hate police until they need an officer or two around in an emergency'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "192fa367-aa13-40f5-8290-0caa9539bc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and ironically racist could they have not come up with somethin better or is it just what white folks do to give them selves pride ooo look at me im being nice for the black people why does it always have to be about race actually the whole world is pussyfootin around for fear of being racist its fuckin daft man'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e747d1-3ed4-4fe7-8182-f4b92435bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(df['wo_stop'])\n",
    "# find all specific symbols which are not in ASCII and no alphabetic\n",
    "symb = re.findall('[^\\x00-\\x7Fa-zA-Z]', text)\n",
    "print(symb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd3440b1-b366-4f2d-bddd-85920b4ec804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['wo_stop'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a17b2d30-982b-40dd-82d6-ef392443181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [people, step, case, nt, people, situation, lu...\n",
       "1      [law, enforcement, trained, shoot, apprehend, ...\n",
       "2      [nt, reckon, black, lives, matter, banners, he...\n",
       "3      [large, number, people, like, police, officers...\n",
       "4      [arab, dude, absolutely, right, shot, extra, t...\n",
       "                             ...                        \n",
       "995                  [remember, sent, national, defence]\n",
       "996    [stats, nt, represent, problem, race, baiting,...\n",
       "997            [quote, mother, wow, hit, hard, accurate]\n",
       "998                                      [video, racist]\n",
       "999                      [god, narrator, annoying, lisp]\n",
       "Name: tokens, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5faa9cec-2363-4320-a188-82cec15642a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_column(tokens):\n",
    "    lemmas = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    return lemmas\n",
    "\n",
    "df['Lemmas'] = df['tokens'].apply(lemmatize_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e884de4-ceef-4395-a605-2f46a6af838e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1340784622.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(df.'Lemmas')\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(df.'Lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "299ba62f-b170-4777-a892-649f4cd1bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [people, step, case, not, people, situation, l...\n",
      "1      [law, enforcement, train, shoot, apprehend, tr...\n",
      "2      [not, reckon, black, life, matter, banner, hol...\n",
      "3      [large, number, people, like, police, officer,...\n",
      "4      [arab, dude, absolutely, right, shot, extra, t...\n",
      "                             ...                        \n",
      "995                  [remember, send, national, defence]\n",
      "996    [stat, not, represent, problem, race, baiting,...\n",
      "997            [quote, mother, wow, hit, hard, accurate]\n",
      "998                                      [video, racist]\n",
      "999                      [god, narrator, annoying, lisp]\n",
      "Name: Lemmas, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e351c040-910b-4386-853d-99abab782ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat',\n",
       "       'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist',\n",
       "       'IsNationalist', 'IsSexist', 'IsHomophobic', 'IsReligiousHate',\n",
       "       'IsRadicalism', 'wo_stop', 'tokens', 'Lemmas'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91849d8e-c75e-4613-b4c9-d6a07436e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_col = df.select_dtypes(include = ['bool'])\n",
    "# Convert boolean columns to integers\n",
    "df[bool_col.columns] = df[bool_col.columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40d9bebc-b54c-4b63-9853-5cc24dd93e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_hate'] = df[['IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene',\n",
    "                    'IsHatespeech', 'IsRacist', 'IsNationalist', 'IsSexist',\n",
    "                    'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']].apply(lambda row: any(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a0ee399-1c38-478d-8fd6-46729d0e5259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentId</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsSexist</th>\n",
       "      <th>IsHomophobic</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>IsRadicalism</th>\n",
       "      <th>wo_stop</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugg2KwwX0V8-aXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>if only people would just take a step back and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>people step case nt people situation lump mess...</td>\n",
       "      <td>[people, step, case, nt, people, situation, lu...</td>\n",
       "      <td>[people, step, case, not, people, situation, l...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugg2s5AzSPioEXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>law enforcement is not trained to shoot to app...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>[law, enforcement, trained, shoot, apprehend, ...</td>\n",
       "      <td>[law, enforcement, train, shoot, apprehend, tr...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugg3dWTOxryFfHgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>dont you reckon them black lives matter banner...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nt reckon black lives matter banners held whit...</td>\n",
       "      <td>[nt, reckon, black, lives, matter, banners, he...</td>\n",
       "      <td>[not, reckon, black, life, matter, banner, hol...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugg7Gd006w1MPngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>there are a very large number of people who do...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large number people like police officers calle...</td>\n",
       "      <td>[large, number, people, like, police, officers...</td>\n",
       "      <td>[large, number, people, like, police, officer,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugg8FfTbbNF8IngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>the arab dude is absolutely right he should ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>arab dude absolutely right shot extra time sho...</td>\n",
       "      <td>[arab, dude, absolutely, right, shot, extra, t...</td>\n",
       "      <td>[arab, dude, absolutely, right, shot, extra, t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CommentId      VideoId  \\\n",
       "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
       "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
       "\n",
       "                                                Text  IsToxic  IsAbusive  \\\n",
       "0  if only people would just take a step back and...        0          0   \n",
       "1  law enforcement is not trained to shoot to app...        1          1   \n",
       "2  dont you reckon them black lives matter banner...        1          1   \n",
       "3  there are a very large number of people who do...        0          0   \n",
       "4  the arab dude is absolutely right he should ha...        0          0   \n",
       "\n",
       "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
       "0         0              0          0             0         0              0   \n",
       "1         0              0          0             0         0              0   \n",
       "2         0              0          1             0         0              0   \n",
       "3         0              0          0             0         0              0   \n",
       "4         0              0          0             0         0              0   \n",
       "\n",
       "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \\\n",
       "0         0             0                0             0   \n",
       "1         0             0                0             0   \n",
       "2         0             0                0             0   \n",
       "3         0             0                0             0   \n",
       "4         0             0                0             0   \n",
       "\n",
       "                                             wo_stop  \\\n",
       "0  people step case nt people situation lump mess...   \n",
       "1  law enforcement trained shoot apprehend traine...   \n",
       "2  nt reckon black lives matter banners held whit...   \n",
       "3  large number people like police officers calle...   \n",
       "4  arab dude absolutely right shot extra time sho...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [people, step, case, nt, people, situation, lu...   \n",
       "1  [law, enforcement, trained, shoot, apprehend, ...   \n",
       "2  [nt, reckon, black, lives, matter, banners, he...   \n",
       "3  [large, number, people, like, police, officers...   \n",
       "4  [arab, dude, absolutely, right, shot, extra, t...   \n",
       "\n",
       "                                              Lemmas  is_hate  \n",
       "0  [people, step, case, not, people, situation, l...    False  \n",
       "1  [law, enforcement, train, shoot, apprehend, tr...     True  \n",
       "2  [not, reckon, black, life, matter, banner, hol...     True  \n",
       "3  [large, number, people, like, police, officer,...    False  \n",
       "4  [arab, dude, absolutely, right, shot, extra, t...    False  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb7019d8-1797-452f-b0fe-802a43039252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_hate'] = df['is_hate'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66d2964a-d4dc-4a8b-ba7b-8cbc47187eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Crear un vectorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transformar la columna 'Lemmas' a vectores TF-IDF\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['Lemmas'].apply(lambda x: ' '.join(x) if x else ''))\n",
    "\n",
    "# Crear un nuevo DataFrame con las características vectorizadas y la columna 'is_hate'\n",
    "df_vectorized = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df_vectorized['is_hate'] = df['is_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb3386ed-a993-4027-ac96-db8e717880ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aaannnyythe', 'ability', 'able', 'aboutdemocrat', 'absolute',\n",
       "       'absolutely', 'absurd', 'abuse', 'academy', 'accelerate',\n",
       "       ...\n",
       "       'youtha', 'youtube', 'ypu', 'yr', 'yup', 'zimmerman', 'zimmermans',\n",
       "       'zionist', 'zone', 'is_hate'],\n",
       "      dtype='object', length=3752)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectorized.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39efc7c4-4506-40fe-835a-bc19f2907b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 355, number of negative: 445\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1185\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.443750 -> initscore=-0.225956\n",
      "[LightGBM] [Info] Start training from score -0.225956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "                 Model  Accuracy  Precision  Recall  F1 Score\n",
      "0  K-Nearest Neighbors     0.605   0.671500   0.605  0.579623\n",
      "1                  SVM     0.695   0.713112   0.695  0.692897\n",
      "2        Random Forest     0.680   0.710659   0.680  0.674505\n",
      "3    Gradient Boosting     0.665   0.733051   0.665  0.648636\n",
      "4             LightGBM     0.685   0.708070   0.685  0.681555\n",
      "5              XGBoost     0.710   0.727473   0.710  0.708256\n",
      "                 Model  Overfitting\n",
      "0  K-Nearest Neighbors      0.18125\n",
      "1                  SVM      0.27250\n",
      "2        Random Forest      0.32000\n",
      "3    Gradient Boosting      0.20000\n",
      "4             LightGBM      0.19625\n",
      "5              XGBoost      0.23625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "X = df_vectorized.drop('is_hate', axis=1)\n",
    "y = df_vectorized['is_hate']\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lista para almacenar los resultados\n",
    "results = []\n",
    "\n",
    "# Lista para almacenar la diferencia entre la precisión en entrenamiento y prueba\n",
    "overfitting_results = []\n",
    "\n",
    "# Definir los modelos\n",
    "models = [\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('SVM', SVC(kernel=\"linear\")),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('LightGBM', LGBMClassifier(n_estimators=100, random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Iterar a través de los modelos\n",
    "for model_name, model in models:\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Realizar predicciones en el conjunto de prueba\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    # Calcular métricas de clasificación en prueba\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
    "    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
    "    f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    \n",
    "    # Calcular métricas de clasificación en entrenamiento\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train, average='weighted')\n",
    "    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n",
    "    f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "    \n",
    "    # Calcular overfitting (diferencia entre precisión en entrenamiento y prueba)\n",
    "    overfitting = accuracy_train - accuracy_test\n",
    "    \n",
    "    # Agregar los resultados a las listas\n",
    "    results.append((model_name, accuracy_test, precision_test, recall_test, f1_test))\n",
    "    overfitting_results.append((model_name, overfitting))\n",
    "\n",
    "# Crear un DataFrame a partir de la lista de resultados\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Crear un DataFrame para la diferencia entre la precisión en entrenamiento y prueba\n",
    "overfitting_df = pd.DataFrame(overfitting_results, columns=['Model', 'Overfitting'])\n",
    "\n",
    "# Imprimir la tabla de resultados\n",
    "print(results_df)\n",
    "\n",
    "# Imprimir la tabla de overfitting\n",
    "print(overfitting_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3d40a-2e34-4680-968f-43c18887faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")  # Carga el modelo de spaCy para español\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Elimina caracteres especiales y puntuación\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    # Convierte a minúsculas\n",
    "    text = text.lower()\n",
    "    # Elimina números\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    # Elimina espacios en blanco adicionales\n",
    "    text = ' '.join(text.split())\n",
    "    # Reemplaza xa0 por espacios en blanco\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "def lemmatize_column(tokens):\n",
    "    lemmas = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    return lemmas\n",
    "\n",
    "def vectorize_text(texts):\n",
    "    # Crear un vectorizador TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    # Transformar los textos a vectores TF-IDF\n",
    "    tfidf_features = tfidf_vectorizer.fit_transform(texts.apply(lambda x: ' '.join(x) if x else ''))\n",
    "    # Crear un DataFrame con las características vectorizadas\n",
    "    df_vectorized = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    return df_vectorized\n",
    "\n",
    "def preprocess_and_vectorize(df):\n",
    "    # Aplicar la limpieza a la columna 'Text'\n",
    "    df['Text'] = df['Text'].apply(preprocess_text)\n",
    "    # Eliminar stopwords\n",
    "    df['wo_stop'] = df['Text'].apply(remove_stopwords)\n",
    "    # Tokenizar el texto\n",
    "    df['tokens'] = df['wo_stop'].apply(tokenize_text)\n",
    "    # Realizar lematización\n",
    "    df['Lemmas'] = df['tokens'].apply(lemmatize_column)\n",
    "    # Vectorizar el texto\n",
    "    df_vectorized = vectorize_text(df['Lemmas'])\n",
    "    # Agregar la columna 'is_hate' al DataFrame vectorizado\n",
    "    df_vectorized['is_hate'] = df['is_hate']\n",
    "    \n",
    "    return df_vectorized\n",
    "\n",
    "# Aplicar todas las transformaciones\n",
    "df_processed = preprocess_and_vectorize(df)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
